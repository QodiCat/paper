这是一篇学术论文中 **“理论分析（THEORETICAL ANALYSIS）” 章节**，核心围绕 **“虚假遗忘（spurious forgetting）” 的数学推导与解释 **，说明模型权重的 “正交更新” 如何导致任务性能变化但未必真正遗忘知识。以下分模块拆解：

### 1. 核心结论（前置总述）

开篇点明：

- 观察到的 **“虚假遗忘”（模型学新任务后，旧任务性能下降但知识未必真丢）**，主要源于 **“模型权重的正交更新”**（权重变化方向与特征主成分近乎正交）。
- 通过分析最终输出的边界（bounds on the shift in the final output），证明 **“冻结底层网络层”** 可缓解该问题。

### 2. 基础定义与假设

#### （1）定义 4.1（残差网络结构）

考虑含残差连接的 *L* 层线性网络，每层定义为：
**X***l*=(**W***l*+**I**)**X***l*−1



- **W***l*∈R*d*×*d*：第 *l* 层权重矩阵
- **I**∈R*d*×*d*：单位矩阵
- **X***l*−1∈R*d*×*n*：第 *l*−1 层输入（特征）

#### （2）假设 4.3（小权重范数）

每层权重矩阵 **W***l* 的范数有上界：
∥**W***l*∥≤*δ*
（*δ* 是小常数，符合大模型 “小权重初始化” 策略，如 GPT 系列）

#### （3）假设 4.4（权重矩阵的扰动）

每层权重矩阵 **W***l* 被扰动为 **W**~*l*=**W***l*+Δ**W***l*，满足：



1. ∥Δ**W***l*∥≤*ϵ*Δ（*ϵ*Δ 是小常数，扰动幅度小）
2. **W***l*⊤Δ**W***l*=0（Δ**W***l* 落在 **W***l* 的左零空间，即权重更新方向与原权重正交 ）

### 3. 核心命题与推导（数学证明 “虚假遗忘” 的根源）

#### （1）命题 4.6（输出偏移的正交性）

若线性映射 **Y**=**WX** 中，**W** 被更新为 **W**~=**W**+Δ**W**（且 Δ**W** 落在 **W** 的零空间），则输出偏移 Δ**Y**=**Y**~−**Y**=Δ**WX** **与 \**Y\** 的列空间中任意向量正交**。



→ 通俗说：权重正交更新时，输出变化方向和原输出 “垂直”，意味着模型输出变了，但未必是 “知识丢了”，更像 “特征方向转了”。

#### （2）命题 4.7（层输出偏移与主成分的近正交性）

在残差网络结构（定义 4.1）和假设 4.3/4.4 下，第 *l* 层输出偏移 Δ**X***l*=**X**~*l*−**X***l* 满足：
⟨Δ**X***l*,**v**1​(**X***l*)⟩≤*O*(*δ*+*ϵ*Δ​)
（**v**1​(**X***l*) 是 **X***l* 的主成分，即最大奇异向量 ）



→ 通俗说：每层输出的变化，和该层特征的 “主成分” 几乎正交。模型学新任务时，特征变化方向没沿着 “关键知识方向” 走，导致任务性能下降（虚假遗忘），但知识本身可能还在。

#### （3）命题 4.9（最终输出的累积偏移正交性）

*L* 层残差网络后，最终输出偏移 **X**~*L*−**X***L* 的边界为：
∥**X**~*L*−**X***L*∥≤*L**ϵ*Δ​(1+*δ*)*L*−1∥**X**0∥



→ 通俗说：层数 *L* 越多，最终输出偏移越大，底层网络的正交更新对顶层输出影响被放大（解释了 “冻结底层可缓解遗忘” 的合理性）。

### 4. 结论与意义（连接理论与实验）

- **虚假遗忘的本质**：模型权重正交更新 → 输出特征偏移与主成分正交 → 任务性能下降，但知识未必丢失（解释了 “为啥有时模型测着差，实际知识还在”）。
- **缓解方法**：冻结底层网络层（减少底层正交更新的累积放大），呼应实验中 “数据重放等方法的效果”，从理论上给持续学习的优化策略（如层冻结、正则化）提供依据。

### 一句话总结

这部分用数学推导证明：**模型权重的 “正交更新” 会让输出特征偏移但未必丢知识，导致 “虚假遗忘”；层数越多偏移越显著，冻结底层可缓解**。 本质是给实验中观察到的 “旧任务损失变化但知识可能保留” 现象，补上数学理论支撑～