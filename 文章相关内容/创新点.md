

可能不成立这个创新点~



​	问题点：模型初始化时需要预先确定任务数量，本质上最开始的时候需要知道固定的任务数（out_dim_list = self.CL_dataset.continual_config['CUR_NUM_CLASS']）。这种预设输出维度的方式导致系统无法适应动态变化的任务需求，本质上是一种"伪持续学习"。与人类大脑的神经可塑性机制形成鲜明对比——海马体等脑区通过终生神经发生（neurogenesis）持续生成新神经元，为学习新任务提供"硬件级"扩展能力，激活对应的神经元。

​	——数量有限，动态添加

​	基于生物神经可塑性机制，我们提出扩展式持续学习方案：

​	方案（1）架构层面针对特点做LLM调整调用，设计LLM特征提取器，网络结构不好改，但提示词融合方式比较简单，利用LLM强大的因果推理和泛化能力激活对应的Lora，如果未见过则进行暂存缓存区训练新的Lora（RAG？）首次引入LLM本身做组件

担心点：引入推理时间加长，针对推理时间长可否做一个优化点做一个小的创新

​	 方案（2）双路自适应神经组合网络组件，MLP组件增加维度，任务偏好损失，特征MLP多层，

