### **Fisher信息矩阵的对角线与参数显著性的关系（以EWC为例）**

#### **1. Fisher信息矩阵（Fisher Information Matrix, FIM）的作用**
Fisher信息矩阵是统计学中衡量“观测数据对模型参数的敏感度”的工具。在深度学习和持续学习（如EWC）中，它被用来评估“每个参数对任务的重要性”，即：
- 如果某个参数的微小变化会显著影响模型的输出（如预测概率或损失值），则该参数对任务很重要。
- 反之，如果参数的变化对模型影响很小，则它相对不重要。

#### **2. Fisher矩阵的对角线元素为什么能表示参数重要性？**
Fisher矩阵是一个对称矩阵，其**对角线元素** \(F_{ii}\) 直接衡量**单个参数 \(\theta_i\) 的重要性**，计算方式为：_
$$
F_{ii} = \mathbb{E}_{x \sim \text{数据}} \left[ \left( \frac{\partial \log p(y|x, \theta)}{\partial \theta_i} \right)^2 \right]
$$


其中：

- $\log p(y|x, \theta)$ 是模型对输入 \(x\) 输出 \(y\) 的对数似然（例如分类任务中的交叉熵损失）。
- $\frac{\partial \log p(y|x, \theta)}{\partial \theta_i}$ 是参数 的梯度（即该参数对损失的影响程度）。

#### **3. 在EWC中如何利用Fisher对角线？**
EWC通过以下方式保护重要参数：
1. **计算旧任务的Fisher对角线**：在旧任务训练完成后，计算所有参数的 \(F_{ii}\)。
2. **正则化约束**：在新任务训练时，对重要参数（高 \(F_{ii}\)）施加更强的约束，防止它们大幅偏离旧任务的最优值，惩罚项更高

#### **4. 为什么用对角线而非完整Fisher矩阵？**

- **计算效率**：完整Fisher矩阵是 \(n \times n\)（\(n\) 为参数量），存储和计算成本极高（例如10亿参数的模型需要 \(10^{18}\) 存储！）。
- **对角近似**：假设参数之间相互独立，仅保留对角线元素，将计算复杂度降至 \(O(n)\)，仍能有效捕捉参数重要性。

